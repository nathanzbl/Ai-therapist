import { useEffect, useRef, useState } from "react";
import ChatLog from "./ChatLog";
import SessionControls from "./SessionControls";
import Header from './header';

export default function App() {
  const [isClient, setIsClient] = useState(false);
  const [isSessionActive, setIsSessionActive] = useState(false);
  const [events, setEvents] = useState([]);
  const [messages, setMessages] = useState([]);
  const [assistantStream, setAssistantStream] = useState("");
  const [localStream, setLocalStream] = useState(null);
  const [totalTokens, setTotalTokens] = useState(0);
  const [contextString, setContextString] = useState("");
  const [contextSummary, setContextSummary] = useState("");
  const assistantBuffer = useRef("");
  const userBuffer = useRef("");
  const currentVoiceMessageId = useRef(null);
  const [dataChannel, setDataChannel] = useState(null);
  const peerConnection = useRef(null);
  const audioElement = useRef(null);
  const [sessionId, setSessionId] = useState(null);
  

  useEffect(() => {
    setIsClient(true);
  }, []);

  // ---- Batched logger ----
  const logBufferRef = useRef([]);
  const flushInFlightRef = useRef(false);
  const flushTimerRef = useRef(null);
  const FLUSH_SIZE = 200;
  const FLUSH_INTERVAL_MS = 15000;

  function logConversation({ sessionId, role, type, message, extras }) {
    if (!sessionId || !type) return;
    logBufferRef.current.push({
      timestamp: new Date().toISOString(),
      sessionId,
      role: role || "system",
      type,
      message: message ?? null,
      extras: extras ?? null,
    });
    if (logBufferRef.current.length >= FLUSH_SIZE) void flushLogs();
  }

  async function flushLogs() {
    if (flushInFlightRef.current) return;
    const batch = logBufferRef.current;
    if (!batch.length) return;
    flushInFlightRef.current = true;
    logBufferRef.current = [];
    try {
      await fetch("/logs/batch", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ records: batch }),
        keepalive: true,
      });
    } catch (err) {
      console.error("Failed to batch log, re-queueing:", err);
      logBufferRef.current = [...batch, ...logBufferRef.current];
    } finally {
      flushInFlightRef.current = false;
    }
  }

  function startPeriodicFlush() {
    if (flushTimerRef.current) return;
    flushTimerRef.current = setInterval(() => void flushLogs(), FLUSH_INTERVAL_MS);
  }
  function stopPeriodicFlush() {
    if (flushTimerRef.current) {
      clearInterval(flushTimerRef.current);
      flushTimerRef.current = null;
    }
  }

  async function startSession() {
    // Get a session token for OpenAI Realtime API
    const tokenResponse = await fetch("/token");
    const data = await tokenResponse.json();
    console.log("Session token data:", data);
    const EPHEMERAL_KEY = data.value;
    setSessionId(data.session.id);

    const trimmedData = {
      ...data.session,
      
      instructions: "[[ OMITTED FOR LOGGING ]]",
      
      
     
    };

    logConversation({
      sessionId: trimmedData.id,
      role: "system",
      type: "session_start",
      message: "Session started",
    });
    logConversation({
      sessionId: trimmedData.id,
      role: "system",
      type: "system",
      message: "Session settings",
      extras: trimmedData, // This will include trimmed session metadata
    });
    // Create a peer connection
    const pc = new RTCPeerConnection();
    // Set up to play remote audio from the model
    audioElement.current = document.createElement("audio");
    audioElement.current.autoplay = true;
    pc.ontrack = (e) => (audioElement.current.srcObject = e.streams[0]);
    // Add local audio track for microphone input in the browser
    const ms = await navigator.mediaDevices.getUserMedia({ audio: true,}); //video: true// });
    setLocalStream(ms);
    pc.addTrack(ms.getTracks()[0]);
    // Set up data channel for sending and receiving events
    const dc = pc.createDataChannel("oai-events");
    setDataChannel(dc);
    // Start the session using the Session Description Protocol (SDP)
    const offer = await pc.createOffer();
    await pc.setLocalDescription(offer);

    const baseUrl = "https://api.openai.com/v1/realtime/calls";
    const model = "gpt-realtime";
    const sdpResponse = await fetch(`${baseUrl}?model=${model}`, {
    method: "POST",
    body: offer.sdp,
    headers: {
        Authorization: `Bearer ${EPHEMERAL_KEY}`,
        "Content-Type": "application/sdp",
    },
});

    const answer = {
      type: "answer",
      sdp: await sdpResponse.text(),
    };
    await pc.setRemoteDescription(answer);

    peerConnection.current = pc;
  }

  async function stopSession() {
    logConversation({ 
      sessionId:sessionId, 
      role: "system", 
      type: "session_end", 
      message: "Session ended",
      extras: { totalTokens }
    });
    stopPeriodicFlush();
    await flushLogs();
    if (dataChannel) {
      dataChannel.close();
    }

    peerConnection.current.getSenders().forEach((sender) => {
      if (sender.track) {
        sender.track.stop();
      }
    });

    if (peerConnection.current) {
      peerConnection.current.close();
    }

    setIsSessionActive(false);
    setDataChannel(null);
    setLocalStream(null);
    setSessionId(null)
    peerConnection.current = null;
  }

  // flush on page unload
  useEffect(() => {
    const handler = () => {
      navigator.sendBeacon?.("/logs/batch", JSON.stringify({ records: logBufferRef.current })) || void flushLogs();
    };
    window.addEventListener("beforeunload", handler);
    return () => window.removeEventListener("beforeunload", handler);
  }, []);


  function sendClientEvent(message) {
    if (dataChannel) {
      const timestamp = new Date().toLocaleTimeString();
      message.event_id = message.event_id || crypto.randomUUID();
      dataChannel.send(JSON.stringify(message));
      if (!message.timestamp) {
        message.timestamp = timestamp;
      }
      setEvents((prev) => [message, ...prev]);
    } else {
      console.error("Failed to send message - no data channel available", message);
    }
  }

  function sendTextMessage(message) {
    const event = {
      type: "conversation.item.create",
      item: {
        type: "message",
        role: "user",
        content: [
          {
            type: "input_text",
            text: message,
          },
        ],
      },
    };

    sendClientEvent(event);
    setMessages((prev) => [
      ...prev,
      { id: crypto.randomUUID(), role: "user", text: message },
    ]);
    setContextString(prev => `${prev}\nUser: ${message}`);
    sendClientEvent({ type: "response.create" });
    logConversation({ sessionId:sessionId, role: "user", type: "chat", message: message });
  }

  function sendInvisiblePrompt(text) {
    const event = {
      type: "conversation.item.create",
      item: {
        type: "message",
        role: "user",
        content: [
          {
            type: "input_text",
            text: text,
          },
        ],
      },
    };

    sendClientEvent(event);
    sendClientEvent({ type: "response.create" });
    logConversation({ sessionId:sessionId, role: "system", type: "system", message: `Initial prompt: ${text}` });
  }

  const fns = {
    stopSession: () => stopSession(),
  };

  useEffect(() => {
    if (dataChannel) {
      dataChannel.addEventListener("message", async (e) => {
        const event = JSON.parse(e.data);
        console.log(event)
        if (!event.timestamp) {
          event.timestamp = new Date().toLocaleTimeString();
        }

        if (event.type === 'response.function_call_arguments.done') {
          const fn = fns[event.name];
          if (fn !== undefined) {
            const args = JSON.parse(event.arguments);
            const result = await fn(args);
            logConversation({ sessionId:sessionId, role: "system", type: "function_call", message: `Function ${event.name} called`, extra: { args, result } });
          }
        }

        if (event.type && event.type.startsWith("response")) {
          if (event.response && event.response.output) {
            event.response.output.forEach((out) => {
              if (out.type === "text") {
                assistantBuffer.current += out.text;
                setAssistantStream(assistantBuffer.current.trim());
              }
            });
          }

          if (event.type === "response.content_part.done") {
            if (event.part?.type === "audio" && event.part.transcript) {
              const assistantMessage = event.part.transcript.trim();
              
              // Check if this is a response to our summary request
              if (totalTokens >= 50000 && assistantMessage.includes("summary") || assistantMessage.includes("Summary")) {
                setContextSummary(assistantMessage);
                console.log('Context summary updated:', assistantMessage);
                // Reset token count after getting summary
                setTotalTokens(0);
                // Clear context string since we have a summary now
                setContextString("");
              } else {
                // Normal message handling
                setMessages((prev) => [
                  ...prev,
                  { id: crypto.randomUUID(), role: "assistant", text: assistantMessage },
                ]);
                setContextString(prev => `${prev}\nAssistant: ${assistantMessage}`);
              }
              
              assistantBuffer.current = "";
              setAssistantStream("");
              logConversation({
                sessionId:sessionId,
                role: "assistant",
                type: "response",
                message: assistantMessage,
              });
            }
          }
        }

        if (event.type === "conversation.item.input_audio_transcription.completed") {
          const transcript = event.transcript;
          if (transcript) {
            const transcriptText = transcript.trim();
            const id = crypto.randomUUID();
            setMessages((prev) => [
              ...prev,
              { id, role: "user", text: transcriptText }
            ]);
            setContextString(prev => `${prev}\nUser: ${transcriptText}`);
            logConversation({ sessionId:sessionId, role: "user", type: "voice", message: transcriptText });
          }
        }

        if (event.type === "response.done") {
          console.log('Received response.done event:', event);
          const usage = event.response?.usage;
          if (usage) {
            console.log('Response usage data:', usage);
            setTotalTokens(prev => {
              const tokensToAdd = usage.total_tokens || 0;
              const newTotal = prev + tokensToAdd;
              console.log(`Token calculation: ${prev} + ${tokensToAdd} = ${newTotal}`);
              return newTotal;
            });
          } else {
            console.log('No usage data in response.done event');
          }
        }
        setEvents((prev) => [event, ...prev]);
      });

      dataChannel.addEventListener("open", () => {
        setIsSessionActive(true);
        setEvents([]);
        setMessages([]);
        setAssistantStream("");
        setContextString("");
        setTotalTokens(0);
        startPeriodicFlush();
        
        sendInvisiblePrompt("Say this phrase exactly: 'Hello! I'm an AI mental health support assistant here to listen and provide encouragement and coping ideas. I am not a licensed therapist or doctor, so I can't diagnose conditions or provide medical advice. Please remember, if you're in crisis, you should call the BYU Counseling and Psychological Services crisis line at (801) 422-3035. Also, please note that your microphone is off by default. If you'd like to talk using voice, you'll need to press the red mic toggle button to turn it on. And if you're comfortable, may I ask for your name?'");
      });
    }
  }, [dataChannel]);

  // Handle high token count by requesting summary
  useEffect(() => {
    if (totalTokens >= 50000) {
      console.log('WARNING - High token usage detected:', totalTokens);
      
      // Send context to AI and request summary
      const summarizePrompt = `Here is the full conversation context. Please create a concise summary of the key points, emotions, and themes discussed. Keep the summary focused on therapeutic relevance. Here's the conversation:\n\n${contextString}\n\nPlease provide a summary that captures the essential context while being much more concise.`;
      
      sendInvisiblePrompt(summarizePrompt);
    }
  }, [totalTokens, contextString]);

  if (!isClient) {
    // Render a placeholder or nothing on the server
    return null;
  }

  return (
    <div className="flex flex-col h-dvh bg-gray-50">
      <Header sessionId={sessionId} />
      <main className="flex-1 flex flex-col items-center overflow-hidden">
        <div className="w-full flex-1 overflow-y-auto p-2 sm:p-4">
          {isSessionActive ? (
            <ChatLog messages={messages} assistantStream={assistantStream} />
          ) : (
            <div className="flex items-center justify-center h-full text-center px-4">
              <p className="text-gray-500 text-xl">
                Press "Start Session" to begin your conversation with the AI Therapist.
              </p>
            </div>
          )}
        </div>
        <div className="w-full max-w-4xl p-2 sm:p-4">
          <SessionControls
            startSession={startSession}
            stopSession={stopSession}
            sendTextMessage={sendTextMessage}
            isSessionActive={isSessionActive}
            localStream={localStream}
          />
        </div>
      </main>
    </div>

  );
}